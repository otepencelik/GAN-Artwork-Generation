{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "img_save_path = 'images-C_dcgan'\n",
    "os.makedirs(img_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
    "#parser.add_argument('--batch_size', type=int, default=64, help='size of the batches')\n",
    "#parser.add_argument('--lr', type=float, default=0.0002, help='adam: learning rate')\n",
    "#parser.add_argument('--beta1', type=float, default=0.5, help='adam: decay of first order momentum of gradient')\n",
    "#parser.add_argument('--beta2', type=float, default=0.999, help='adam: decay of second order momentum of gradient')\n",
    "#parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "#parser.add_argument('--latent_dim', type=int, default=100, help='dimensionality of the latent space')\n",
    "#parser.add_argument('--n_classes', type=int, default=10, help='number of classes for dataset')\n",
    "#parser.add_argument('--img_size', type=int, default=32, help='size of each image dimension')\n",
    "#parser.add_argument('--channels', type=int, default=1, help='number of image channels')\n",
    "#parser.add_argument('--sample_interval', type=int, default=400, help='interval between image sampling')\n",
    "#args = parser.parse_args()\n",
    "#print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parameter_opt():\n",
    "    None\n",
    "args = parameter_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.n_epochs=100\n",
    "args.batch_size = 64\n",
    "args.lr = 0.0001\n",
    "args.beta1 = 0.5\n",
    "args.beta2 = 0.999\n",
    "args.n_cpu = 8\n",
    "args.latent_dim = 150\n",
    "args.ngf = 32\n",
    "args.ndf = 8\n",
    "args.n_classes = 27\n",
    "args.img_size = 64\n",
    "args.channels = 3\n",
    "args.sample_interval = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C,H,W = args.channels, args.img_size, args.img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.deconv1_1 = nn.ConvTranspose2d(args.latent_dim, d*2, 4, 1, 0)\n",
    "        self.deconv1_1_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv1_2 = nn.ConvTranspose2d(args.n_classes, d*2, 4, 1, 0)\n",
    "        self.deconv1_2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(d)\n",
    "        self.deconv4 = nn.ConvTranspose2d(d, d, 4, 2, 1) ## might change d \n",
    "        self.deconv4_bn = nn.BatchNorm2d(d) ## might change d \n",
    "        self.deconv5 = nn.ConvTranspose2d(d, C, 4, 2, 1)\n",
    "\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input, label):\n",
    "        x = F.relu(self.deconv1_1_bn(self.deconv1_1(input)))\n",
    "        y = F.relu(self.deconv1_2_bn(self.deconv1_2(label)))\n",
    "        x = torch.cat([x, y], 1)\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "        x = F.tanh(self.deconv5(x))\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(C, d//2, 4, 2, 1)\n",
    "        self.conv1_2 = nn.Conv2d(args.n_classes, d//2, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv5 = nn.Conv2d(d * 8, 1, 4, 1, 0)\n",
    "\n",
    "\n",
    "    # def forward(self, input):\n",
    "    def forward(self, input, label):\n",
    "        x = F.leaky_relu(self.conv1_1(input), 0.2)\n",
    "        y = F.leaky_relu(self.conv1_2(label), 0.2)\n",
    "        x = torch.cat([x, y], 1)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize Generator and discriminator\n",
    "generator = Generator(args.ngf)\n",
    "discriminator = Discriminator(args.ndf)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (deconv1_1): ConvTranspose2d(150, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (deconv1_1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv1_2): ConvTranspose2d(27, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (deconv1_2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv3_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv4): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv4_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv5): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1_1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(27, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(64, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1_1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(27, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(64, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure data loader\n",
    "#os.makedirs('../../data', exist_ok=True)\n",
    "#dataloader = torch.utils.data.DataLoader(\n",
    "#    datasets.MNIST('../../data', train=True, download=True,\n",
    "#                   transform=transforms.Compose([\n",
    "#                       transforms.Resize(args.img_size),\n",
    "#                       transforms.ToTensor(),\n",
    "#                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#                   ])),\n",
    "#    batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "#print('the data is ok')\n",
    "\n",
    "from dataloader_wikiart import *\n",
    "\n",
    "dataloader = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=args.lr, betas=(args.beta1, args.beta2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=args.lr, betas=(args.beta1, args.beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iocak/.local/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/iocak/.local/lib/python3.7/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/1273] [D loss: 1.334643] [G loss: 0.781792]\n",
      "[Epoch 0/100] [Batch 200/1273] [D loss: 0.048684] [G loss: 3.135690]\n",
      "[Epoch 0/100] [Batch 400/1273] [D loss: 0.003493] [G loss: 6.970459]\n",
      "[Epoch 0/100] [Batch 600/1273] [D loss: 0.001811] [G loss: 6.943761]\n",
      "[Epoch 0/100] [Batch 800/1273] [D loss: 0.012333] [G loss: 8.255661]\n",
      "[Epoch 0/100] [Batch 1000/1273] [D loss: 0.002319] [G loss: 6.355018]\n",
      "[Epoch 0/100] [Batch 1200/1273] [D loss: 0.000522] [G loss: 9.268084]\n",
      "[Epoch 1/100] [Batch 127/1273] [D loss: 0.003620] [G loss: 5.989457]\n",
      "[Epoch 1/100] [Batch 327/1273] [D loss: 0.000272] [G loss: 8.617294]\n",
      "[Epoch 1/100] [Batch 527/1273] [D loss: 0.000333] [G loss: 10.289530]\n",
      "[Epoch 1/100] [Batch 727/1273] [D loss: 0.000183] [G loss: 9.418334]\n",
      "[Epoch 1/100] [Batch 927/1273] [D loss: 0.000468] [G loss: 8.085608]\n",
      "[Epoch 1/100] [Batch 1127/1273] [D loss: 0.000249] [G loss: 8.785481]\n",
      "[Epoch 2/100] [Batch 54/1273] [D loss: 0.000303] [G loss: 8.928316]\n",
      "[Epoch 2/100] [Batch 254/1273] [D loss: 0.000276] [G loss: 10.070114]\n",
      "[Epoch 2/100] [Batch 454/1273] [D loss: 0.000219] [G loss: 9.204243]\n",
      "[Epoch 2/100] [Batch 654/1273] [D loss: 0.000232] [G loss: 10.088960]\n",
      "[Epoch 2/100] [Batch 854/1273] [D loss: 0.000057] [G loss: 10.934596]\n",
      "[Epoch 2/100] [Batch 1054/1273] [D loss: 0.000082] [G loss: 10.179114]\n",
      "[Epoch 2/100] [Batch 1254/1273] [D loss: 0.000075] [G loss: 10.162142]\n",
      "[Epoch 3/100] [Batch 181/1273] [D loss: 0.000099] [G loss: 9.597561]\n",
      "[Epoch 3/100] [Batch 381/1273] [D loss: 0.000059] [G loss: 12.212455]\n",
      "[Epoch 3/100] [Batch 581/1273] [D loss: 0.000067] [G loss: 10.872097]\n",
      "[Epoch 3/100] [Batch 781/1273] [D loss: 0.000051] [G loss: 11.657272]\n",
      "[Epoch 3/100] [Batch 981/1273] [D loss: 0.000017] [G loss: 13.010572]\n",
      "[Epoch 3/100] [Batch 1181/1273] [D loss: 0.000063] [G loss: 11.891043]\n",
      "[Epoch 4/100] [Batch 108/1273] [D loss: 0.000033] [G loss: 12.127822]\n",
      "[Epoch 4/100] [Batch 308/1273] [D loss: 0.000036] [G loss: 10.559198]\n",
      "[Epoch 4/100] [Batch 508/1273] [D loss: 0.000157] [G loss: 8.910870]\n",
      "[Epoch 4/100] [Batch 708/1273] [D loss: 0.000039] [G loss: 10.833630]\n",
      "[Epoch 4/100] [Batch 908/1273] [D loss: 0.000041] [G loss: 11.737038]\n",
      "[Epoch 4/100] [Batch 1108/1273] [D loss: 0.000012] [G loss: 12.374039]\n",
      "[Epoch 5/100] [Batch 35/1273] [D loss: 0.000013] [G loss: 12.080215]\n",
      "[Epoch 5/100] [Batch 235/1273] [D loss: 0.000033] [G loss: 12.448213]\n",
      "[Epoch 5/100] [Batch 435/1273] [D loss: 0.000013] [G loss: 12.419284]\n",
      "[Epoch 5/100] [Batch 635/1273] [D loss: 0.000020] [G loss: 13.275296]\n",
      "[Epoch 5/100] [Batch 835/1273] [D loss: 0.000019] [G loss: 11.244345]\n",
      "[Epoch 5/100] [Batch 1035/1273] [D loss: 0.000010] [G loss: 13.332467]\n",
      "[Epoch 5/100] [Batch 1235/1273] [D loss: 0.000016] [G loss: 11.866066]\n",
      "[Epoch 6/100] [Batch 162/1273] [D loss: 0.000007] [G loss: 14.023042]\n",
      "[Epoch 6/100] [Batch 362/1273] [D loss: 0.000008] [G loss: 12.758276]\n",
      "[Epoch 6/100] [Batch 562/1273] [D loss: 0.000004] [G loss: 13.774026]\n",
      "[Epoch 6/100] [Batch 762/1273] [D loss: 0.000007] [G loss: 12.654680]\n",
      "[Epoch 6/100] [Batch 962/1273] [D loss: 0.000005] [G loss: 13.162605]\n",
      "[Epoch 6/100] [Batch 1162/1273] [D loss: 0.000003] [G loss: 14.444090]\n",
      "[Epoch 7/100] [Batch 89/1273] [D loss: 0.000004] [G loss: 13.539919]\n",
      "[Epoch 7/100] [Batch 289/1273] [D loss: 0.000005] [G loss: 13.220995]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d98a6f39b3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen_y_for_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mbatches_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        Batch_Size = imgs.shape[0]\n",
    "        N_Class = args.n_classes\n",
    "        img_size = args.img_size\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(torch.ones(Batch_Size).cuda(), requires_grad=False)\n",
    "        fake = Variable(torch.zeros(Batch_Size).cuda(), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(torch.FloatTensor).cuda())\n",
    "\n",
    "        real_y = torch.zeros(Batch_Size, N_Class)\n",
    "        real_y = real_y.scatter_(1, labels.view(Batch_Size, 1), 1).view(Batch_Size, N_Class, 1, 1).contiguous()\n",
    "        real_y = Variable(real_y.expand(-1, -1, img_size, img_size).cuda())\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        noise = Variable(torch.randn((Batch_Size, args.latent_dim,1,1)).cuda())\n",
    "        gen_labels = (torch.rand(Batch_Size, 1) * N_Class).type(torch.LongTensor)\n",
    "        gen_y = torch.zeros(Batch_Size, N_Class)\n",
    "        gen_y = Variable(gen_y.scatter_(1, gen_labels.view(Batch_Size, 1), 1).view(Batch_Size, N_Class,1,1).cuda())\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        # Loss for real images\n",
    "        d_real_loss = adversarial_loss(discriminator(real_imgs + 0.0 * torch.randn(real_imgs.shape, device=device), # add noise \n",
    "                                                     real_y).squeeze(), valid)\n",
    "        # Loss for fake images\n",
    "        gen_imgs = generator(noise, gen_y)\n",
    "        gen_y_for_D = gen_y.view(Batch_Size, N_Class, 1, 1).contiguous().expand(-1, -1, img_size, img_size)\n",
    "\n",
    "        d_fake_loss = adversarial_loss(discriminator(gen_imgs.detach(),gen_y_for_D).squeeze(), fake)\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss)\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs,gen_y_for_D).squeeze(), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % 200 == 0:\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, args.n_epochs, i, len(dataloader),\n",
    "                                                                              d_loss.data.cpu(), g_loss.data.cpu()))\n",
    "        D_losses.append(d_loss.data.cpu())\n",
    "        G_losses.append(g_loss.data.cpu())\n",
    "\n",
    "        if batches_done % args.sample_interval == 0:\n",
    "            noise = Variable(torch.FloatTensor(np.random.normal(0, 1, (N_Class**2, args.latent_dim,1,1))).cuda())\n",
    "            #fixed labels\n",
    "            y_ = torch.LongTensor(np.array([num for num in range(N_Class)])).view(N_Class,1).expand(-1,N_Class).contiguous()\n",
    "            y_fixed = torch.zeros(N_Class**2, N_Class)\n",
    "            y_fixed = Variable(y_fixed.scatter_(1,y_.view(N_Class**2,1),1).view(N_Class**2, N_Class,1,1).cuda())\n",
    "\n",
    "            gen_imgs = generator(noise, y_fixed).view(-1,C,H,W)\n",
    "\n",
    "            save_image(gen_imgs.data, img_save_path + '/%d-%d.png' % (epoch,batches_done), nrow=N_Class, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
